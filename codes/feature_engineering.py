#!/usr/bin/env python
# -*- encoding: utf-8 -*-
"""
@File    :   feature_engineering.py
@Contact :   3289218653@qq.com
@License :   (C)Copyright 2021-2022 PowerLZY

@Modify Time      @Author           @Version    @Desciption
------------      -------           --------    -----------
2021-10-04 17:17   PowerLZY&yuan_mes  1.0         None
"""
import sys
import os

from tqdm import tqdm
import joblib
from codes.features import *
from codes.utils import *

from sklearn.feature_extraction.text import TfidfVectorizer

from gensim.models import Word2Vec, KeyedVectors
from gensim.models.word2vec import PathLineSentences

def feature_engine(obj, sample_path, inter_path):
    """ Feature engineering for different features. """
    dirs = sample_path.split('/')
    # Data belongs to 'train' or 'test' and file belongs to 'pe' or 'asm'
    data_type, file_type = dirs[-2], dirs[-1]
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()
    arr = np.zeros((len(filename), obj.dim))

    if file_type == 'pe':
        with tqdm(total=len(filename), ncols=80, desc=f"{data_type}_{obj.name}") as pbar:
            for i, sample in enumerate(filename):
                with open(f"{sample_path}/{sample}", "rb") as f:
                    bytez = f.read()
                arr[i, :] = obj.feature_vector(bytez)
                pbar.update(1)
    else:  # file_type == 'asm'
        with tqdm(total=len(filename), ncols=80, desc=obj.name) as pbar:
            for i, sample in enumerate(filename):
                with open(f"{sample_path}/{sample}.asm", "rb") as f:
                    stringz = f.read().decode('utf-8', errors='ignore')
                arr[i, :] = obj.feature_vector(stringz)
                pbar.update(1)

    np.save(f"{inter_path}/feature/{data_type}_{obj.name}.npy", arr)


def feature_tfidf_df(obj, sample_path, inter_path):
    """ Save the words of all samples to a DataFrame fot tf-idf input. """
    dirs = sample_path.split('/')
    data_type, file_type = dirs[-2], dirs[-1]
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()
    if file_type == 'asm':
        filename = [f + '.asm' for f in filename]
    all_word_feature = []
    with tqdm(total=len(filename), ncols=80, desc=f"{obj.name_tfidf}_{data_type}") as pbar:
        for i, sample in enumerate(filename):
            if(obj.name_tfidf == 'ins'):
                with open(f"{sample_path}/{sample}", "r", encoding='utf-8', errors='ignore') as f:
                    all_word_feature.append(obj.tfidf_features(f))
            else:
                with open(f"{sample_path}/{sample}", "rb") as f:
                    btyez = f.read()
                all_word_feature.append(obj.tfidf_features(btyez))
            pbar.update(1)

    word_feature = pd.DataFrame({"word_feature": all_word_feature})
    if data_type == 'test':
        word_feature.to_csv(f"{inter_path}/feature/test_{obj.name_tfidf}_tfidf.csv", index=False)

    return word_feature


def model_tfidf(obj, sample_path, inter_path, stop_words=None, ngram_range=(1, 1), max_features = 1000):
    train_words_ = feature_tfidf_df(obj, sample_path, inter_path)
    test_words_ = feature_tfidf_df(obj, sample_path.replace('train', 'test'), inter_path)
    all_words_ = train_words_.append(test_words_)

    vectorizer = TfidfVectorizer(stop_words, ngram_range)
    # TODO time ？
    vectorizer.fit(all_words_.word_feature.tolist())
    words = vectorizer.transform(train_words_.word_feature.tolist())

    np.save(f"{inter_path}/feature/train_{obj.name_tfidf}_{max_features}.npy", words.toarray())
    joblib.dump(vectorizer, open(f"{inter_path}/models/TFIDF_model_{obj.name_tfidf}.pth", "wb"))


def feature_tfidf_np(name_tfidf, inter_path, max_features):
    vectorizer = joblib.load(open(f"{inter_path}/models/TFIDF_model_{name_tfidf}.pth", "rb"))
    vectorizer.max_features = max_features
    words_ = pd.read_csv(f"{inter_path}/test_{name_tfidf}_tfidf.csv")
    words = vectorizer.transform(words_.word_feature.tolist())
    np.save(f"{inter_path}/feature/test_{name_tfidf}_{max_features}.npy", words.toarray())


def feature_asm2txt(sample_path, inter_path):
    dirs = sample_path.split('/')
    data_type, file_type = dirs[-2], dirs[-1]

    def asm2txt_by_datatype(spath, dtype):
        with open(f"{inter_path}/{dtype}_filename.txt", 'r') as fp:
            filenames = fp.read().split()

        if dtype == 'test':
            spath = spath.replace('train', 'test')

        with tqdm(total=len(filenames), ncols=80, desc=f"{dtype}_asm2txt") as pbar:
            for filename in filenames:
                with open(os.path.join(spath, filename) + '.asm', "r", encoding='utf-8', errors='ignore') as fp:
                    opline_list = OpcodeInfo().asm_to_txt(fp)
                f = open(os.path.join(f"{inter_path}/semantic/", filename) + '.txt', 'w+')
                for line in opline_list:
                    f.write(line)
                    f.write('\n')
                f.close()
                pbar.update(1)

    asm2txt_by_datatype(sample_path, data_type)
    asm2txt_by_datatype(sample_path, 'test')

    print(f"{inter_path}/semantic data saved!")


def feature_asm2vec(data_type, inter_path):
    """Feature engineering for asm2vec feature."""
    if data_type == "train":
        # TODO : 模型空判断
        # Train a Word2vec model by mixing traing set and test set
        sentences = PathLineSentences(f"{inter_path}/semantic/")
        model = Word2Vec(sentences=sentences, vector_size=1024, window=5, min_count=5, workers=5)
        model.wv.save_word2vec_format(f"{inter_path}/models/asm2vec.bin", binary=True, sort_attr='count')

    # Load the trained Word2vec model
    model_wv = KeyedVectors.load_word2vec_format(f"{inter_path}/models/asm2vec.bin", binary=True)

    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()
    # Feature engineering for generating string vector features
    obj = StringVector()
    arr = np.zeros((len(filename), obj.dim))
    with tqdm(total=len(filename), ncols=80, desc=obj.name) as pbar:
        for i, file in enumerate(filename):
            with open(f"{inter_path}/semantic/{file}.txt", "rb") as f:
                stringz = f.read().decode('utf-8', errors='ignore')
            lines = ' '.join(stringz.split('\n'))
            raw_words = list(set(lines.split()))
            arr[i, :] = obj.feature_vector((model_wv, raw_words))
            pbar.update(1)
    arr[np.isnan(arr)] = 0
    np.save(f"{inter_path}/feature/{data_type}_asm2vec_.npy", arr)


def feature_engineering(data_type, data_path, inter_path):
    # data_path = '../data/raw_data'
    # data_type = "train"
    # inter_path = '../data/user_data'

    pe_path = f"{data_path}/{data_type}/pe"
    asm_path = f"{data_path}/{data_type}/asm"

    """
    if data_type == 'train':
        #fix_file_index(data_path, inter_path)

        # ------------------------ words&ins ------------------------
        #model_tfidf(StringExtractor(), pe_path, inter_path)
        #model_tfidf(OpcodeInfo(), asm_path, inter_path, stop_words=[';'], ngram_range=(1, 3))

        # ------------------------ semantic ------------------------
        feature_asm2txt(asm_path, inter_path)

    else:  # data_type == 'test'
        # ------------------------ words&ins ------------------------
        feature_tfidf_np('words', inter_path, max_features=300)
        feature_tfidf_np('ins', inter_path, max_features=1000)
    

    pe_objs = [ByteHistogram(), ByteEntropyHistogram(), StringExtractor()]
    for obj in pe_objs:
        feature_engine(obj, pe_path, inter_path)

    asm_objs = [SectionInfo(), ImportsInfo(), ExportsInfo()]
    for obj in asm_objs:
        feature_engine(obj, asm_path, inter_path)

    # ------------------------ semantic ------------------------
    #feature_asm2vec(data_type, inter_path)  # data_type
    """
    # TODO 特征融合 'ember_section_ins_words', 'ember_section_ins_semantic'

    fused_label = 'ember'
    features = ['histogram', 'byteentropy', 'strings']
    data_type = ['train', 'test']
    arr = []
    for f in features:
        arr.append(np.load(f"{inter_path}/feature/train_{f}.npy"))
    np.save(f"{inter_path}/feature/train_{fused_label}.npy", np.hstack(arr).astype(np.float32))